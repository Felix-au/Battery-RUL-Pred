# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/195K-xunVuRoI4rrz2zyd4TCL68JzEwWJ
"""

import pandas as pd

# Load dataset
df = pd.read_csv("Battery_RUL.csv")

# Display basic info
print("Shape of data:", df.shape)
print("\nData types:\n", df.dtypes)
print("\nFirst 5 rows:\n", df.head())
print("\nMissing values:\n", df.isnull().sum())

import matplotlib.pyplot as plt
import seaborn as sns

# Summary statistics
print(df.describe())

# Correlation heatmap
plt.figure(figsize=(12,8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

# Pairplot for selected features (optional)
# sns.pairplot(df[['Voltage', 'Current', 'Temperature', 'RUL']])

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Drop any irrelevant columns if needed
# df.drop(['ID'], axis=1, inplace=True)
#identify?
#ir

# Handle missing values (if any)
df = df.dropna()

# Define features and target
X = df.drop('RUL', axis=1)
y = df['RUL']

# Split into train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Linear Regression Model

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Train model
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)

# Predict
y_pred_lr = lr.predict(X_test_scaled)

# Evaluate
mse_lr = mean_squared_error(y_test, y_pred_lr)
rmse_lr = np.sqrt(mse_lr)
r2_lr = r2_score(y_test, y_pred_lr)

# Display results
print("ðŸ“Š Linear Regression Performance:")
print(f"RMSE: {rmse_lr:.2f}")
print(f"RÂ² Score: {r2_lr:.4f}")

plt.figure(figsize=(12, 5))
plt.plot(y_test.values[:100], label='Actual RUL', marker='o')
plt.plot(y_pred_lr[:100], label='Predicted RUL - Linear Regression', marker='x')
plt.title("Linear Regression: Actual vs Predicted RUL")
plt.xlabel("Sample Index")
plt.ylabel("Remaining Useful Life")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

# Train model
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# Predict
y_pred_knn = knn.predict(X_test_scaled)

# Evaluate
r2_knn = r2_score(y_test, y_pred_knn)
mse_knn = mean_squared_error(y_test, y_pred_knn)
rmse_knn = np.sqrt(mse_knn)

print(f"KNN - RÂ²: {r2_knn:.4f}, RMSE: {rmse_knn:.4f}")

plt.figure(figsize=(12, 5))
plt.plot(y_test.values[:100], label='Actual RUL', marker='o')
plt.plot(y_pred_knn[:100], label='Predicted RUL - KNN', marker='x', linestyle='--')
plt.title("KNN: Actual vs Predicted RUL")
plt.xlabel("Sample Index")
plt.ylabel("Remaining Useful Life")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Train model
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)

# Predict
y_pred_rf = rf.predict(X_test_scaled)

# Evaluate
mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)
r2_rf = r2_score(y_test, y_pred_rf)

# Display results
print("Random Forest Regressor Performance:")
print(f"RMSE: {rmse_rf:.2f}")
print(f"RÂ² Score: {r2_rf:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.plot(y_test.values[:100], label='Actual RUL', marker='o')
plt.plot(y_pred_rf[:100], label='Predicted RUL - Random Forest', marker='x')
plt.title("Random Forest: Actual vs Predicted RUL")
plt.xlabel("Sample Index")
plt.ylabel("Remaining Useful Life")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""### XGBoost Regressor"""

import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Train model
xgbr = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgbr.fit(X_train_scaled, y_train)

# Predict
y_pred_xgb = xgbr.predict(X_test_scaled)

# Evaluate
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mse_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

# Display results
print("XGBoost Regressor Performance:")
print(f"RMSE: {rmse_xgb:.2f}")
print(f"RÂ² Score: {r2_xgb:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.plot(y_test.values[:100], label='Actual RUL', marker='o')
plt.plot(y_pred_xgb[:100], label='Predicted RUL - XGBoost', marker='x')
plt.title("XGBoost: Actual vs Predicted RUL")
plt.xlabel("Sample Index")
plt.ylabel("Remaining Useful Life")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Support Vector Regressor"""

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Train model
svr = SVR()
svr.fit(X_train_scaled, y_train)

# Predict
y_pred_svr = svr.predict(X_test_scaled)

# Evaluate
mse_svr = mean_squared_error(y_test, y_pred_svr)
rmse_svr = np.sqrt(mse_svr)
r2_svr = r2_score(y_test, y_pred_svr)

# Display results
print("Support Vector Regressor Performance:")
print(f"RMSE: {rmse_svr:.2f}")
print(f"RÂ² Score: {r2_svr:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.plot(y_test.values[:100], label='Actual RUL', marker='o')
plt.plot(y_pred_svr[:100], label='Predicted RUL - SVR', marker='x')
plt.title("SVR: Actual vs Predicted RUL")
plt.xlabel("Sample Index")
plt.ylabel("Remaining Useful Life")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Gradient Boosting Regressor"""

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf']
}

# Create a GridSearchCV object
grid = GridSearchCV(SVR(), param_grid, refit=True, verbose=2)

# Fit the grid search to the data
grid.fit(X_train_scaled, y_train)

# Print the best parameters
print("Best parameters found: ", grid.best_params_)

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Train model using the best parameters found from GridSearchCV
# Best parameters found: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}
svr_tuned = SVR(C=100, gamma=0.001, kernel='rbf')
svr_tuned.fit(X_train_scaled, y_train)

# Predict
y_pred_svr_tuned = svr_tuned.predict(X_test_scaled)

# Evaluate
mse_svr_tuned = mean_squared_error(y_test, y_pred_svr_tuned)
rmse_svr_tuned = np.sqrt(mse_svr_tuned)
r2_svr_tuned = r2_score(y_test, y_pred_svr_tuned)

# Display results
print("Support Vector Regressor (Tuned) Performance:")
print(f"RMSE: {rmse_svr_tuned:.2f}")
print(f"RÂ² Score: {r2_svr_tuned:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.plot(y_test.values[:100], label='Actual RUL', marker='o')

plt.plot(y_pred_svr_tuned[:100], label='Predicted RUL - SVR (Tuned)', marker='x')
plt.title("SVR (Tuned): Actual vs Predicted RUL")
plt.xlabel("Sample Index")
plt.ylabel("Remaining Useful Life")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.ensemble import GradientBoostingRegressor

# Train model
gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)
gbr.fit(X_train_scaled, y_train)

# Predict
y_pred_gbr = gbr.predict(X_test_scaled)

# Evaluate
mse_gbr = mean_squared_error(y_test, y_pred_gbr)
rmse_gbr = np.sqrt(mse_gbr)
r2_gbr = r2_score(y_test, y_pred_gbr)

# Display results
print("Gradient Boosting Regressor Performance:")
print(f"RMSE: {rmse_gbr:.2f}")
print(f"RÂ² Score: {r2_gbr:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.plot(y_test.values[:100], label='Actual RUL', marker='o')
plt.plot(y_pred_gbr[:100], label='Predicted RUL - Gradient Boosting', marker='x')
plt.title("Gradient Boosting: Actual vs Predicted RUL")
plt.xlabel("Sample Index")
plt.ylabel("Remaining Useful Life")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd

# Create a DataFrame to store the results
results_df = pd.DataFrame({
    'Model': ['Linear Regression', 'KNN', 'Random Forest', 'XGBoost', 'SVR (Tuned)', 'Gradient Boosting'],
    'RMSE': [rmse_lr, rmse_knn, rmse_rf, rmse_xgb, rmse_svr_tuned, rmse_gbr],
    'RÂ² Score': [r2_lr, r2_knn, r2_rf, r2_xgb, r2_svr_tuned, r2_gbr]
})

# Create a final results table for comparison
final_results = results_df.sort_values(by='RMSE')

# Display the final results table
print("Final Model Comparison:")
display(final_results)

import matplotlib.pyplot as plt

# Create a larger bar chart to visualize the comparison for better clarity
results_df_sorted = results_df.sort_values(by='RMSE')
ax = results_df_sorted.plot(kind='bar', figsize=(15, 8), width=0.8)
plt.title('Model Performance Comparison', fontsize=16)
plt.xlabel("Model")
plt.ylabel("Score")
plt.xticks(range(len(results_df_sorted)), results_df_sorted['Model'], rotation=45, ha='right') # Set model names as x-tick labels
plt.legend(title="Metric")
plt.grid(axis='y')
plt.tight_layout()
plt.show()